,notes,train_acc,train_loss,val_acc,val_loss
0,,0.52,1.5702,0.36,2.6146
1,,0.61,1.1658,0.48,1.6689
2,,0.67,1.0387,0.55,1.3087
3,,0.70,0.8847,0.52,2.1685
4,,0.68,1.0551,0.63,1.2116
5,,0.75,0.7416,0.57,1.6092
6,,0.73,0.8106,0.65,1.2004
7,,0.74,0.8076,0.68,1.0835
8,,0.76,0.7628,0.65,1.2304
9,,0.75,0.7767,0.60,1.8365
10,,0.74,0.8850,0.62,1.7981
11,,0.79,0.6725,0.64,1.6278
12,,0.73,0.8328,0.45,2.2432
13,,0.73,0.9653,0.60,1.6301
14,,0.79,0.7053,0.59,1.6580
15,,0.77,0.7192,0.69,1.2970
16,,0.79,0.7203,0.63,1.8269
17,,0.78,0.6923,0.52,2.3936
18,,0.76,0.8077,0.59,1.6875
19,,0.77,0.8149,0.64,1.5544
20,,0.79,0.6655,0.73,1.1019
21,,0.77,0.7878,0.70,1.4738
22,,0.79,0.7901,0.60,1.8485
23,,0.78,0.7912,0.73,1.0796
24,,0.77,0.8384,0.70,1.5219
25,,0.83,0.5897,0.71,1.5127
26,,0.81,0.6131,0.63,1.3624
27,,0.80,0.6626,0.58,1.7904
28,,0.77,0.8335,0.72,1.0781
29,,0.80,0.7082,0.65,1.9514
30,,0.78,0.7455,0.72,1.7035
31,,0.79,0.7519,0.66,1.3291
32,,0.79,0.7602,0.68,1.0290
33,,0.81,0.6740,0.69,1.1186
34,,0.80,0.7116,0.70,1.2249
35,,0.78,0.7894,0.66,1.7031
36,,0.81,0.6503,0.69,1.4267
37,,0.78,0.8188,0.69,1.9376
38,,0.80,0.7369,0.62,1.9519
39,created new Adam optimizer with LR: 0.00100000000000000002,0.81,0.7124,0.64,1.3779
40,,0.78,0.8013,0.63,1.7866
41,,0.82,0.5992,0.61,1.6228
42,,0.81,0.6371,0.70,1.0368
43,,0.83,0.5512,0.72,1.4736
44,,0.82,0.5834,0.70,1.5726
45,,0.84,0.5032,0.70,1.3060
46,,0.83,0.5213,0.64,1.7438
47,,0.82,0.5572,0.76,1.0244
48,,0.83,0.5336,0.68,1.2160
49,,0.86,0.4344,0.71,1.1603
50,,0.85,0.4729,0.70,1.2165
51,,0.84,0.5125,0.70,1.1081
52,,0.85,0.4586,0.66,1.1367
53,,0.85,0.4500,0.70,1.2860
54,,0.83,0.5067,0.70,1.2482
55,,0.85,0.4911,0.66,1.2291
56,,0.82,0.5613,0.65,1.3451
57,,0.86,0.4420,0.71,1.0428
58,,0.86,0.4314,0.64,1.6846
59,,0.86,0.4341,0.70,1.1738
60,,0.84,0.5097,0.68,1.2022
61,,0.83,0.5010,0.68,1.2173
62,,0.84,0.4896,0.71,1.2681
63,,0.86,0.4592,0.69,1.0241
64,,0.84,0.4754,0.69,1.1263
65,,0.86,0.4213,0.72,1.0319
66,,0.84,0.5113,0.68,1.2075
67,,0.83,0.5165,0.70,1.2872
68,,0.84,0.4489,0.70,1.4598
69,,0.85,0.4928,0.69,1.2040
70,,0.86,0.4215,0.68,1.4743
71,,0.85,0.4537,0.68,1.0567
72,,0.85,0.4526,0.72,1.5643
73,,0.87,0.3907,0.65,1.5122
74,,0.84,0.4799,0.70,1.2489
75,,0.85,0.4634,0.73,1.3766
76,,0.84,0.4811,0.70,1.0574
77,,0.82,0.5279,0.70,1.0729
78,,0.86,0.4282,0.73,1.1159
79,created new Adam optimizer with LR: 0.0001,0.84,0.5019,0.64,1.6925
80,,0.83,0.5024,0.68,1.2551
81,,0.85,0.4296,0.69,1.2246
82,,0.84,0.4621,0.69,1.2785
83,,0.86,0.3785,0.68,1.2360
84,,0.86,0.4433,0.72,1.0429
85,,0.86,0.4086,0.73,1.2012
86,,0.86,0.4019,0.70,1.4272
87,,0.85,0.4290,0.67,1.6499
88,,0.84,0.4713,0.68,1.4423
89,,0.85,0.4519,0.69,1.1938
90,,0.88,0.3732,0.72,1.0540
91,,0.86,0.4196,0.73,0.9499
92,,0.86,0.3793,0.70,1.2524
93,,0.85,0.4379,0.74,1.1426
94,,0.86,0.4346,0.68,1.1630
95,,0.85,0.4138,0.71,1.1729
96,,0.85,0.4242,0.67,1.3193
97,,0.86,0.4064,0.70,1.0208
98,,0.85,0.4503,0.77,1.0414
99,,0.84,0.4386,0.72,0.8729
