,notes,train_acc,train_loss,val_acc,val_loss
0,,0.57,1.5809,0.31,3.6939
1,,0.64,1.2403,0.38,1.8780
2,,0.68,1.0561,0.50,1.7509
3,,0.69,1.0078,0.59,1.4087
4,,0.71,1.0440,0.64,1.3163
5,,0.74,0.8550,0.55,1.8820
6,,0.76,0.8428,0.53,1.5348
7,,0.73,0.9220,0.57,2.2926
8,,0.74,0.9714,0.57,1.7937
9,,0.76,0.8388,0.66,1.4942
10,,0.76,0.8457,0.64,1.1396
11,,0.75,0.8818,0.55,1.7059
12,,0.78,0.7720,0.68,1.0134
13,,0.76,0.8836,0.64,1.4927
14,,0.79,0.7299,0.51,1.8355
15,,0.77,0.7859,0.61,1.4167
16,,0.79,0.7642,0.73,0.9407
17,,0.80,0.7258,0.68,1.5776
18,,0.79,0.7730,0.60,1.7883
19,,0.78,0.8130,0.51,1.7860
20,,0.80,0.7009,0.55,1.5796
21,,0.80,0.7491,0.68,1.1723
22,,0.79,0.7315,0.66,1.3222
23,,0.80,0.7022,0.61,1.1836
24,,0.81,0.7070,0.55,1.8331
25,,0.80,0.7677,0.51,2.0293
26,,0.80,0.7781,0.71,1.5901
27,,0.80,0.8904,0.62,1.2725
28,,0.81,0.6876,0.72,1.0360
29,,0.79,0.7546,0.51,1.9068
30,,0.81,0.7259,0.66,1.5881
31,,0.81,0.8224,0.68,1.1489
32,,0.84,0.6473,0.72,1.0505
33,,0.80,0.7668,0.49,2.0253
34,,0.79,0.8560,0.63,2.0078
35,,0.79,0.8478,0.65,1.2920
36,,0.83,0.6047,0.70,1.0673
37,,0.85,0.6136,0.67,1.3274
38,,0.81,0.7897,0.66,1.2642
39,created new Adam optimizer with LR: 0.00100000000000000002,0.82,0.6831,0.68,1.3351
40,,0.83,0.6910,0.48,2.8574
41,,0.81,0.7162,0.74,0.9563
42,,0.85,0.5480,0.74,0.9067
43,,0.85,0.5070,0.75,0.9569
44,,0.84,0.5464,0.75,0.9227
45,,0.86,0.5003,0.78,0.8734
46,,0.86,0.4703,0.74,0.8843
47,,0.85,0.5193,0.73,0.8766
48,,0.85,0.4928,0.73,0.8843
49,,0.85,0.5223,0.77,0.8095
50,,0.85,0.5321,0.77,0.7851
51,,0.86,0.4743,0.78,0.8400
52,,0.86,0.4910,0.74,0.8342
53,,0.85,0.4756,0.76,0.8159
54,,0.85,0.5036,0.74,0.8024
55,,0.84,0.5298,0.78,0.7896
56,,0.86,0.4664,0.77,0.7881
57,,0.86,0.4209,0.73,0.8147
58,,0.86,0.4550,0.77,0.7597
59,,0.86,0.4674,0.79,0.7668
60,,0.85,0.4622,0.77,0.7616
61,,0.86,0.4473,0.77,0.7419
62,,0.86,0.4926,0.77,0.7527
63,,0.86,0.4589,0.76,0.7603
64,,0.86,0.4472,0.77,0.7845
65,,0.86,0.4512,0.76,0.7753
66,,0.87,0.4454,0.75,0.7779
67,,0.86,0.4439,0.76,0.7762
68,,0.85,0.4755,0.77,0.7686
69,,0.86,0.4521,0.75,0.7948
70,,0.85,0.4588,0.77,0.7685
71,,0.87,0.4169,0.76,0.8255
72,,0.86,0.4455,0.71,0.9023
73,,0.86,0.4641,0.75,0.7613
74,,0.85,0.4675,0.76,0.7678
75,,0.87,0.4216,0.77,0.7752
76,,0.86,0.4492,0.76,0.7523
77,,0.86,0.4471,0.77,0.7612
78,,0.86,0.4130,0.74,0.7641
79,created new Adam optimizer with LR: 0.0001,0.86,0.4250,0.77,0.7499
80,,0.86,0.4162,0.76,0.7472
81,,0.87,0.3787,0.76,0.7460
82,,0.88,0.3757,0.78,0.7498
83,,0.88,0.3532,0.77,0.7524
84,,0.86,0.4146,0.78,0.7456
85,,0.87,0.4068,0.78,0.7438
86,,0.86,0.4097,0.77,0.7459
87,,0.88,0.3606,0.77,0.7467
88,,0.85,0.4514,0.78,0.7495
89,,0.87,0.3894,0.76,0.7445
90,,0.88,0.3810,0.75,0.7445
91,,0.86,0.3879,0.77,0.7444
92,,0.88,0.3747,0.77,0.7456
93,,0.86,0.4216,0.78,0.7392
94,,0.87,0.3721,0.77,0.7382
95,,0.88,0.3621,0.78,0.7403
96,,0.88,0.3862,0.76,0.7367
97,,0.86,0.4262,0.78,0.7338
98,,0.86,0.4281,0.77,0.7374
99,,0.86,0.4242,0.77,0.7371
