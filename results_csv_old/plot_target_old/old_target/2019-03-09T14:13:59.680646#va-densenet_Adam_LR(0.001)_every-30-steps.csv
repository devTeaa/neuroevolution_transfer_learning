,notes,train_acc,train_loss,val_acc,val_loss
0,,0.48,1.5856,0.16,2.4629
1,,0.62,1.1322,0.24,1.9637
2,,0.63,1.0966,0.42,4.8493
3,,0.63,1.0146,0.16,3.5305
4,,0.66,0.9961,0.42,4.4191
5,,0.69,0.8565,0.42,4.6847
6,,0.68,0.9310,0.02,4.2273
7,,0.70,0.8110,0.24,3.8406
8,,0.75,0.6833,0.25,5.2149
9,,0.73,0.7635,0.43,6.4795
10,,0.73,0.7219,0.43,3.7447
11,,0.72,0.6786,0.24,3.0019
12,,0.74,0.7201,0.16,10.1046
13,,0.74,0.6968,0.16,5.1230
14,,0.74,0.7564,0.43,8.0945
15,,0.76,0.7089,0.24,6.3348
16,,0.78,0.5937,0.24,3.4792
17,,0.75,0.6981,0.24,12.0179
18,,0.76,0.6535,0.17,5.6559
19,,0.77,0.5902,0.30,4.9945
20,,0.77,0.6654,0.24,10.1561
21,,0.78,0.5815,0.16,9.4443
22,,0.80,0.5150,0.43,7.4049
23,,0.75,0.7224,0.24,8.2206
24,,0.80,0.5280,0.16,12.5698
25,,0.81,0.4960,0.24,5.6789
26,,0.81,0.5312,0.24,9.8157
27,,0.78,0.6194,0.16,9.3296
28,,0.80,0.5773,0.24,7.3997
29,created new Adam optimizer with LR: 0.00100000000000000002,0.80,0.5256,0.24,9.4275
30,,0.81,0.5258,0.24,7.3820
31,,0.80,0.5563,0.38,5.4850
32,,0.82,0.4466,0.25,10.5359
33,,0.86,0.3555,0.25,5.9329
34,,0.84,0.4081,0.25,5.7991
35,,0.85,0.3711,0.24,8.6052
36,,0.82,0.4494,0.25,8.8413
37,,0.83,0.4272,0.42,8.7411
38,,0.82,0.4406,0.25,7.1092
39,,0.84,0.4327,0.24,6.1421
40,,0.83,0.4231,0.25,9.6626
41,,0.85,0.3917,0.25,6.8504
42,,0.86,0.3662,0.24,5.0656
43,,0.81,0.4978,0.43,7.3642
44,,0.86,0.3769,0.25,6.7396
45,,0.86,0.3563,0.23,5.8537
46,,0.84,0.4078,0.24,12.0070
47,,0.85,0.3686,0.24,7.3805
48,,0.84,0.4347,0.24,6.7018
49,,0.83,0.4098,0.43,9.5291
50,,0.85,0.3671,0.25,9.5448
51,,0.85,0.3952,0.25,6.2242
52,,0.82,0.4262,0.27,8.0022
53,,0.82,0.4673,0.25,8.1774
54,,0.84,0.4123,0.42,7.0120
55,,0.88,0.3183,0.24,7.9935
56,,0.86,0.3519,0.25,7.8087
57,,0.85,0.3859,0.24,8.0529
58,,0.85,0.3953,0.25,9.9275
59,created new Adam optimizer with LR: 0.0001,0.85,0.3668,0.24,8.2928
60,,0.83,0.4486,0.24,4.4936
61,,0.88,0.3127,0.41,5.5724
62,,0.87,0.3448,0.25,9.9449
63,,0.87,0.3327,0.24,10.1996
64,,0.86,0.3464,0.25,7.8257
65,,0.87,0.3315,0.43,3.2022
66,,0.88,0.3258,0.24,7.2399
67,,0.86,0.3425,0.24,9.4582
68,,0.86,0.3637,0.43,6.0701
69,,0.89,0.2817,0.25,7.3676
70,,0.87,0.3325,0.25,9.8005
71,,0.87,0.3284,0.25,8.0809
72,,0.87,0.3398,0.24,3.9093
73,,0.87,0.3411,0.24,11.0696
74,,0.84,0.3769,0.24,3.3235
75,,0.84,0.3893,0.24,6.9221
76,,0.87,0.3434,0.43,6.5512
77,,0.86,0.3444,0.25,8.4495
78,,0.85,0.3802,0.25,6.5667
79,,0.84,0.4141,0.25,9.6296
80,,0.87,0.3569,0.24,8.7230
81,,0.86,0.3716,0.44,8.0264
82,,0.87,0.3421,0.24,10.4638
83,,0.87,0.3368,0.25,8.5320
84,,0.87,0.3390,0.24,8.2796
85,,0.84,0.3717,0.25,7.6344
86,,0.85,0.3522,0.24,7.8589
87,,0.88,0.3251,0.24,11.9887
88,,0.89,0.2902,0.23,5.5537
89,created new Adam optimizer with LR: 0.00001,0.86,0.3614,0.24,7.8417
90,,0.87,0.3477,0.24,8.0872
91,,0.83,0.3989,0.43,6.5485
92,,0.87,0.3443,0.25,5.8991
93,,0.83,0.4323,0.42,8.5742
94,,0.88,0.3346,0.24,9.9432
95,,0.84,0.3996,0.24,6.4512
96,,0.88,0.3068,0.25,10.2775
97,,0.86,0.3835,0.25,5.7962
98,,0.85,0.3925,0.43,11.7899
99,,0.88,0.3269,0.24,9.6416
