Model: va-densenet Optimizer: Adam LR: 0.01 Weight Decay: 0.0001 Steps Decay: 30 

Epoch: 1 
train epoch 1: acc 0.43 loss 1.7214 with data size 243 
val epoch 1: acc 0.10 loss 2.1800 with data size 80 
==============================================
Epoch: 2 
train epoch 2: acc 0.49 loss 1.4806 with data size 243 
val epoch 2: acc 0.29 loss 1.7087 with data size 80 
==============================================
Epoch: 3 
train epoch 3: acc 0.51 loss 1.3590 with data size 243 
val epoch 3: acc 0.20 loss 2.5932 with data size 80 
==============================================
Epoch: 4 
train epoch 4: acc 0.54 loss 1.2936 with data size 243 
val epoch 4: acc 0.38 loss 1.5807 with data size 80 
==============================================
Epoch: 5 
train epoch 5: acc 0.55 loss 1.2266 with data size 243 
val epoch 5: acc 0.51 loss 1.3296 with data size 80 
==============================================
Epoch: 6 
train epoch 6: acc 0.53 loss 1.2655 with data size 243 
val epoch 6: acc 0.34 loss 1.8327 with data size 80 
==============================================
Epoch: 7 
train epoch 7: acc 0.62 loss 1.0094 with data size 243 
val epoch 7: acc 0.24 loss 2.0590 with data size 80 
==============================================
Epoch: 8 
train epoch 8: acc 0.64 loss 0.9716 with data size 243 
val epoch 8: acc 0.45 loss 1.5696 with data size 80 
==============================================
Epoch: 9 
train epoch 9: acc 0.58 loss 1.0762 with data size 243 
val epoch 9: acc 0.42 loss 1.3946 with data size 80 
==============================================
Epoch: 10 
train epoch 10: acc 0.68 loss 0.8639 with data size 243 
val epoch 10: acc 0.38 loss 1.6957 with data size 80 
==============================================
Epoch: 11 
train epoch 11: acc 0.62 loss 0.9812 with data size 243 
val epoch 11: acc 0.56 loss 1.0179 with data size 80 
==============================================
Epoch: 12 
train epoch 12: acc 0.62 loss 0.9871 with data size 243 
val epoch 12: acc 0.51 loss 1.2713 with data size 80 
==============================================
Epoch: 13 
train epoch 13: acc 0.64 loss 0.9298 with data size 243 
val epoch 13: acc 0.49 loss 1.7849 with data size 80 
==============================================
Epoch: 14 
train epoch 14: acc 0.66 loss 0.9236 with data size 243 
val epoch 14: acc 0.56 loss 1.3389 with data size 80 
==============================================
Epoch: 15 
train epoch 15: acc 0.66 loss 0.9265 with data size 243 
val epoch 15: acc 0.45 loss 1.4583 with data size 80 
==============================================
Epoch: 16 
train epoch 16: acc 0.72 loss 0.7777 with data size 243 
val epoch 16: acc 0.51 loss 1.2676 with data size 80 
==============================================
Epoch: 17 
train epoch 17: acc 0.69 loss 0.7971 with data size 243 
val epoch 17: acc 0.50 loss 1.4786 with data size 80 
==============================================
Epoch: 18 
train epoch 18: acc 0.68 loss 0.8820 with data size 243 
val epoch 18: acc 0.50 loss 1.3951 with data size 80 
==============================================
Epoch: 19 
train epoch 19: acc 0.69 loss 0.8364 with data size 243 
val epoch 19: acc 0.29 loss 2.2421 with data size 80 
==============================================
Epoch: 20 
train epoch 20: acc 0.66 loss 0.9196 with data size 243 
val epoch 20: acc 0.47 loss 1.7017 with data size 80 
==============================================
Epoch: 21 
train epoch 21: acc 0.74 loss 0.6904 with data size 243 
val epoch 21: acc 0.21 loss 2.7031 with data size 80 
==============================================
Epoch: 22 
train epoch 22: acc 0.67 loss 0.9367 with data size 243 
val epoch 22: acc 0.41 loss 1.5351 with data size 80 
==============================================
Epoch: 23 
train epoch 23: acc 0.72 loss 0.7388 with data size 243 
val epoch 23: acc 0.29 loss 2.5221 with data size 80 
==============================================
Epoch: 24 
train epoch 24: acc 0.72 loss 0.7820 with data size 243 
val epoch 24: acc 0.61 loss 1.4369 with data size 80 
==============================================
Epoch: 25 
train epoch 25: acc 0.74 loss 0.6991 with data size 243 
val epoch 25: acc 0.47 loss 2.3670 with data size 80 
==============================================
Epoch: 26 
train epoch 26: acc 0.68 loss 0.8499 with data size 243 
val epoch 26: acc 0.50 loss 1.5294 with data size 80 
==============================================
Epoch: 27 
train epoch 27: acc 0.72 loss 0.7347 with data size 243 
val epoch 27: acc 0.41 loss 1.7356 with data size 80 
==============================================
Epoch: 28 
train epoch 28: acc 0.76 loss 0.6486 with data size 243 
val epoch 28: acc 0.55 loss 1.1642 with data size 80 
==============================================
Epoch: 29 
train epoch 29: acc 0.73 loss 0.7316 with data size 243 
val epoch 29: acc 0.49 loss 1.8384 with data size 80 
==============================================
Epoch: 30 
created new Adam optimizer with LR: 0.01train epoch 30: acc 0.74 loss 0.7207 with data size 243 
val epoch 30: acc 0.55 loss 1.5523 with data size 80 
==============================================
Epoch: 31 
created new Adam optimizer with LR: 0.001train epoch 31: acc 0.73 loss 0.6889 with data size 243 
val epoch 31: acc 0.46 loss 1.6920 with data size 80 
==============================================
Epoch: 32 
created new Adam optimizer with LR: 0.001train epoch 32: acc 0.76 loss 0.6257 with data size 243 
val epoch 32: acc 0.57 loss 1.1454 with data size 80 
==============================================
Epoch: 33 
created new Adam optimizer with LR: 0.001train epoch 33: acc 0.78 loss 0.5818 with data size 243 
val epoch 33: acc 0.57 loss 1.0773 with data size 80 
==============================================
Epoch: 34 
created new Adam optimizer with LR: 0.001train epoch 34: acc 0.77 loss 0.5821 with data size 243 
val epoch 34: acc 0.53 loss 1.2133 with data size 80 
==============================================
Epoch: 35 
created new Adam optimizer with LR: 0.001train epoch 35: acc 0.77 loss 0.5956 with data size 243 
val epoch 35: acc 0.53 loss 1.2338 with data size 80 
==============================================
Epoch: 36 
created new Adam optimizer with LR: 0.001train epoch 36: acc 0.78 loss 0.5732 with data size 243 
val epoch 36: acc 0.61 loss 1.1890 with data size 80 
==============================================
Epoch: 37 
created new Adam optimizer with LR: 0.001train epoch 37: acc 0.76 loss 0.5837 with data size 243 
val epoch 37: acc 0.62 loss 1.0661 with data size 80 
==============================================
Epoch: 38 
created new Adam optimizer with LR: 0.001train epoch 38: acc 0.78 loss 0.5814 with data size 243 
val epoch 38: acc 0.53 loss 1.2168 with data size 80 
==============================================
Epoch: 39 
created new Adam optimizer with LR: 0.001train epoch 39: acc 0.79 loss 0.5699 with data size 243 
val epoch 39: acc 0.61 loss 1.0836 with data size 80 
==============================================
Epoch: 40 
created new Adam optimizer with LR: 0.001train epoch 40: acc 0.80 loss 0.5183 with data size 243 
val epoch 40: acc 0.56 loss 1.3176 with data size 80 
==============================================
Epoch: 41 
created new Adam optimizer with LR: 0.001train epoch 41: acc 0.80 loss 0.5520 with data size 243 
val epoch 41: acc 0.56 loss 1.1785 with data size 80 
==============================================
Epoch: 42 
created new Adam optimizer with LR: 0.001train epoch 42: acc 0.78 loss 0.5533 with data size 243 
val epoch 42: acc 0.51 loss 1.4906 with data size 80 
==============================================
Epoch: 43 
created new Adam optimizer with LR: 0.001train epoch 43: acc 0.80 loss 0.5246 with data size 243 
val epoch 43: acc 0.55 loss 1.2370 with data size 80 
==============================================
Epoch: 44 
created new Adam optimizer with LR: 0.001train epoch 44: acc 0.79 loss 0.5745 with data size 243 
val epoch 44: acc 0.46 loss 1.4602 with data size 80 
==============================================
Epoch: 45 
created new Adam optimizer with LR: 0.001train epoch 45: acc 0.82 loss 0.5279 with data size 243 
val epoch 45: acc 0.61 loss 1.0937 with data size 80 
==============================================
Epoch: 46 
created new Adam optimizer with LR: 0.001train epoch 46: acc 0.76 loss 0.6234 with data size 243 
val epoch 46: acc 0.50 loss 1.3398 with data size 80 
==============================================
Epoch: 47 
created new Adam optimizer with LR: 0.001train epoch 47: acc 0.79 loss 0.5749 with data size 243 
val epoch 47: acc 0.59 loss 1.2281 with data size 80 
==============================================
Epoch: 48 
created new Adam optimizer with LR: 0.001train epoch 48: acc 0.80 loss 0.5594 with data size 243 
val epoch 48: acc 0.57 loss 1.2063 with data size 80 
==============================================
Epoch: 49 
created new Adam optimizer with LR: 0.001train epoch 49: acc 0.78 loss 0.5499 with data size 243 
val epoch 49: acc 0.64 loss 1.2251 with data size 80 
==============================================
Epoch: 50 
created new Adam optimizer with LR: 0.001train epoch 50: acc 0.77 loss 0.5895 with data size 243 
val epoch 50: acc 0.53 loss 1.2812 with data size 80 
==============================================
Epoch: 51 
created new Adam optimizer with LR: 0.001train epoch 51: acc 0.80 loss 0.5332 with data size 243 
val epoch 51: acc 0.54 loss 1.1677 with data size 80 
==============================================
Epoch: 52 
created new Adam optimizer with LR: 0.001train epoch 52: acc 0.78 loss 0.5731 with data size 243 
val epoch 52: acc 0.53 loss 1.2256 with data size 80 
==============================================
Epoch: 53 
created new Adam optimizer with LR: 0.001train epoch 53: acc 0.78 loss 0.5757 with data size 243 
val epoch 53: acc 0.51 loss 1.1935 with data size 80 
==============================================
Epoch: 54 
created new Adam optimizer with LR: 0.001train epoch 54: acc 0.79 loss 0.5348 with data size 243 
val epoch 54: acc 0.60 loss 1.1576 with data size 80 
==============================================
Epoch: 55 
created new Adam optimizer with LR: 0.001train epoch 55: acc 0.79 loss 0.5550 with data size 243 
val epoch 55: acc 0.55 loss 1.2182 with data size 80 
==============================================
Epoch: 56 
created new Adam optimizer with LR: 0.001train epoch 56: acc 0.79 loss 0.5512 with data size 243 
val epoch 56: acc 0.53 loss 1.1740 with data size 80 
==============================================
Epoch: 57 
created new Adam optimizer with LR: 0.001train epoch 57: acc 0.80 loss 0.5341 with data size 243 
val epoch 57: acc 0.60 loss 1.2308 with data size 80 
==============================================
Epoch: 58 
created new Adam optimizer with LR: 0.001train epoch 58: acc 0.82 loss 0.5304 with data size 243 
val epoch 58: acc 0.56 loss 1.3753 with data size 80 
==============================================
Epoch: 59 
created new Adam optimizer with LR: 0.001train epoch 59: acc 0.78 loss 0.5640 with data size 243 
val epoch 59: acc 0.53 loss 1.3231 with data size 80 
==============================================
Epoch: 60 
created new Adam optimizer with LR: 0.001train epoch 60: acc 0.80 loss 0.5478 with data size 243 
val epoch 60: acc 0.57 loss 1.3601 with data size 80 
==============================================
Epoch: 61 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 61: acc 0.76 loss 0.5945 with data size 243 
val epoch 61: acc 0.57 loss 1.0679 with data size 80 
==============================================
Epoch: 62 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 62: acc 0.80 loss 0.5239 with data size 243 
val epoch 62: acc 0.59 loss 1.1769 with data size 80 
==============================================
Epoch: 63 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 63: acc 0.80 loss 0.5131 with data size 243 
val epoch 63: acc 0.61 loss 1.0337 with data size 80 
==============================================
Epoch: 64 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 64: acc 0.79 loss 0.5413 with data size 243 
val epoch 64: acc 0.51 loss 1.1935 with data size 80 
==============================================
Epoch: 65 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 65: acc 0.77 loss 0.5680 with data size 243 
val epoch 65: acc 0.57 loss 1.2299 with data size 80 
==============================================
Epoch: 66 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 66: acc 0.83 loss 0.4775 with data size 243 
val epoch 66: acc 0.61 loss 1.1097 with data size 80 
==============================================
Epoch: 67 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 67: acc 0.81 loss 0.5143 with data size 243 
val epoch 67: acc 0.57 loss 1.3023 with data size 80 
==============================================
Epoch: 68 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 68: acc 0.82 loss 0.5062 with data size 243 
val epoch 68: acc 0.55 loss 1.2599 with data size 80 
==============================================
Epoch: 69 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 69: acc 0.76 loss 0.6005 with data size 243 
val epoch 69: acc 0.55 loss 1.2018 with data size 80 
==============================================
Epoch: 70 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 70: acc 0.78 loss 0.5815 with data size 243 
val epoch 70: acc 0.57 loss 1.4395 with data size 80 
==============================================
Epoch: 71 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 71: acc 0.80 loss 0.5423 with data size 243 
val epoch 71: acc 0.55 loss 1.4096 with data size 80 
==============================================
Epoch: 72 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 72: acc 0.80 loss 0.5397 with data size 243 
val epoch 72: acc 0.51 loss 1.4474 with data size 80 
==============================================
Epoch: 73 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 73: acc 0.81 loss 0.5342 with data size 243 
val epoch 73: acc 0.51 loss 1.3214 with data size 80 
==============================================
Epoch: 74 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 74: acc 0.79 loss 0.5443 with data size 243 
val epoch 74: acc 0.56 loss 1.1220 with data size 80 
==============================================
Epoch: 75 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 75: acc 0.81 loss 0.5093 with data size 243 
val epoch 75: acc 0.55 loss 1.2682 with data size 80 
==============================================
Epoch: 76 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 76: acc 0.80 loss 0.5306 with data size 243 
val epoch 76: acc 0.57 loss 1.2005 with data size 80 
==============================================
Epoch: 77 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 77: acc 0.80 loss 0.5315 with data size 243 
val epoch 77: acc 0.56 loss 1.2118 with data size 80 
==============================================
Epoch: 78 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 78: acc 0.79 loss 0.5847 with data size 243 
val epoch 78: acc 0.57 loss 1.0842 with data size 80 
==============================================
Epoch: 79 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 79: acc 0.76 loss 0.5906 with data size 243 
val epoch 79: acc 0.53 loss 1.3780 with data size 80 
==============================================
Epoch: 80 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 80: acc 0.81 loss 0.5257 with data size 243 
val epoch 80: acc 0.57 loss 1.1515 with data size 80 
==============================================
Epoch: 81 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 81: acc 0.82 loss 0.5017 with data size 243 
val epoch 81: acc 0.55 loss 1.2921 with data size 80 
==============================================
Epoch: 82 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 82: acc 0.83 loss 0.4939 with data size 243 
val epoch 82: acc 0.56 loss 1.1538 with data size 80 
==============================================
Epoch: 83 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 83: acc 0.82 loss 0.5104 with data size 243 
val epoch 83: acc 0.55 loss 1.4031 with data size 80 
==============================================
Epoch: 84 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 84: acc 0.81 loss 0.5043 with data size 243 
val epoch 84: acc 0.59 loss 1.2393 with data size 80 
==============================================
Epoch: 85 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 85: acc 0.79 loss 0.5387 with data size 243 
val epoch 85: acc 0.59 loss 1.1169 with data size 80 
==============================================
Epoch: 86 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 86: acc 0.81 loss 0.5232 with data size 243 
val epoch 86: acc 0.61 loss 1.1200 with data size 80 
==============================================
Epoch: 87 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 87: acc 0.80 loss 0.5389 with data size 243 
val epoch 87: acc 0.60 loss 1.2511 with data size 80 
==============================================
Epoch: 88 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 88: acc 0.81 loss 0.5118 with data size 243 
val epoch 88: acc 0.54 loss 1.0610 with data size 80 
==============================================
Epoch: 89 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 89: acc 0.80 loss 0.5378 with data size 243 
val epoch 89: acc 0.62 loss 1.1251 with data size 80 
==============================================
Epoch: 90 
created new Adam optimizer with LR: 0.00010000000000000002train epoch 90: acc 0.80 loss 0.5448 with data size 243 
val epoch 90: acc 0.56 loss 1.2588 with data size 80 
==============================================
Epoch: 91 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 91: acc 0.82 loss 0.4969 with data size 243 
val epoch 91: acc 0.62 loss 1.2368 with data size 80 
==============================================
Epoch: 92 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 92: acc 0.78 loss 0.5555 with data size 243 
val epoch 92: acc 0.50 loss 1.3285 with data size 80 
==============================================
Epoch: 93 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 93: acc 0.81 loss 0.5163 with data size 243 
val epoch 93: acc 0.55 loss 1.2043 with data size 80 
==============================================
Epoch: 94 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 94: acc 0.80 loss 0.5336 with data size 243 
val epoch 94: acc 0.47 loss 1.4094 with data size 80 
==============================================
Epoch: 95 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 95: acc 0.81 loss 0.5115 with data size 243 
val epoch 95: acc 0.53 loss 1.1663 with data size 80 
==============================================
Epoch: 96 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 96: acc 0.81 loss 0.5146 with data size 243 
val epoch 96: acc 0.51 loss 1.2221 with data size 80 
==============================================
Epoch: 97 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 97: acc 0.78 loss 0.5535 with data size 243 
val epoch 97: acc 0.56 loss 1.3135 with data size 80 
==============================================
Epoch: 98 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 98: acc 0.81 loss 0.4980 with data size 243 
val epoch 98: acc 0.59 loss 1.1820 with data size 80 
==============================================
Epoch: 99 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 99: acc 0.79 loss 0.5654 with data size 243 
val epoch 99: acc 0.57 loss 1.1991 with data size 80 
==============================================
Epoch: 100 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 100: acc 0.82 loss 0.5013 with data size 243 
val epoch 100: acc 0.60 loss 1.2134 with data size 80 
==============================================
