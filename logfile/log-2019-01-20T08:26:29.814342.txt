Model: va-densenet Optimizer: Adam LR: 0.001 Weight Decay: 0.0001 Steps Decay: 20 

Epoch: 1 
train epoch 1: acc 0.42 loss 1.6114 with data size 243 
val epoch 1: acc 0.42 loss 1.5596 with data size 80 
==============================================
Epoch: 2 
train epoch 2: acc 0.41 loss 1.5605 with data size 243 
val epoch 2: acc 0.42 loss 1.5379 with data size 80 
==============================================
Epoch: 3 
train epoch 3: acc 0.42 loss 1.5481 with data size 243 
val epoch 3: acc 0.44 loss 1.5182 with data size 80 
==============================================
Epoch: 4 
train epoch 4: acc 0.45 loss 1.5176 with data size 243 
val epoch 4: acc 0.42 loss 1.5023 with data size 80 
==============================================
Epoch: 5 
train epoch 5: acc 0.45 loss 1.4969 with data size 243 
val epoch 5: acc 0.42 loss 1.5046 with data size 80 
==============================================
Epoch: 6 
train epoch 6: acc 0.43 loss 1.4832 with data size 243 
val epoch 6: acc 0.46 loss 1.4365 with data size 80 
==============================================
Epoch: 7 
train epoch 7: acc 0.46 loss 1.4437 with data size 243 
val epoch 7: acc 0.51 loss 1.4557 with data size 80 
==============================================
Epoch: 8 
train epoch 8: acc 0.47 loss 1.4543 with data size 243 
val epoch 8: acc 0.44 loss 1.4228 with data size 80 
==============================================
Epoch: 9 
train epoch 9: acc 0.47 loss 1.4341 with data size 243 
val epoch 9: acc 0.45 loss 1.4144 with data size 80 
==============================================
Epoch: 10 
train epoch 10: acc 0.46 loss 1.4048 with data size 243 
val epoch 10: acc 0.50 loss 1.4187 with data size 80 
==============================================
Epoch: 11 
train epoch 11: acc 0.49 loss 1.3991 with data size 243 
val epoch 11: acc 0.42 loss 1.5408 with data size 80 
==============================================
Epoch: 12 
train epoch 12: acc 0.47 loss 1.3893 with data size 243 
val epoch 12: acc 0.45 loss 1.3862 with data size 80 
==============================================
Epoch: 13 
train epoch 13: acc 0.48 loss 1.3612 with data size 243 
val epoch 13: acc 0.45 loss 1.3762 with data size 80 
==============================================
Epoch: 14 
train epoch 14: acc 0.51 loss 1.3477 with data size 243 
val epoch 14: acc 0.51 loss 1.3523 with data size 80 
==============================================
Epoch: 15 
train epoch 15: acc 0.49 loss 1.3514 with data size 243 
val epoch 15: acc 0.46 loss 1.3998 with data size 80 
==============================================
Epoch: 16 
train epoch 16: acc 0.49 loss 1.3280 with data size 243 
val epoch 16: acc 0.45 loss 1.4086 with data size 80 
==============================================
Epoch: 17 
train epoch 17: acc 0.49 loss 1.3138 with data size 243 
val epoch 17: acc 0.49 loss 1.3447 with data size 80 
==============================================
Epoch: 18 
train epoch 18: acc 0.51 loss 1.3130 with data size 243 
val epoch 18: acc 0.45 loss 1.3812 with data size 80 
==============================================
Epoch: 19 
train epoch 19: acc 0.52 loss 1.3216 with data size 243 
val epoch 19: acc 0.47 loss 1.3342 with data size 80 
==============================================
Epoch: 20 
created new Adam optimizer with LR: 0.001train epoch 20: acc 0.52 loss 1.2785 with data size 243 
val epoch 20: acc 0.53 loss 1.3173 with data size 80 
==============================================
Epoch: 21 
train epoch 21: acc 0.51 loss 1.2806 with data size 243 
val epoch 21: acc 0.46 loss 1.3642 with data size 80 
==============================================
Epoch: 22 
train epoch 22: acc 0.50 loss 1.2594 with data size 243 
val epoch 22: acc 0.53 loss 1.3197 with data size 80 
==============================================
Epoch: 23 
train epoch 23: acc 0.51 loss 1.2350 with data size 243 
val epoch 23: acc 0.51 loss 1.3221 with data size 80 
==============================================
Epoch: 24 
train epoch 24: acc 0.50 loss 1.2657 with data size 243 
val epoch 24: acc 0.54 loss 1.2965 with data size 80 
==============================================
Epoch: 25 
train epoch 25: acc 0.53 loss 1.2231 with data size 243 
val epoch 25: acc 0.50 loss 1.3212 with data size 80 
==============================================
Epoch: 26 
train epoch 26: acc 0.53 loss 1.2356 with data size 243 
val epoch 26: acc 0.50 loss 1.2819 with data size 80 
==============================================
Epoch: 27 
train epoch 27: acc 0.50 loss 1.2636 with data size 243 
val epoch 27: acc 0.45 loss 1.3476 with data size 80 
==============================================
Epoch: 28 
train epoch 28: acc 0.51 loss 1.2393 with data size 243 
val epoch 28: acc 0.51 loss 1.2642 with data size 80 
==============================================
Epoch: 29 
train epoch 29: acc 0.54 loss 1.2178 with data size 243 
val epoch 29: acc 0.49 loss 1.3250 with data size 80 
==============================================
Epoch: 30 
train epoch 30: acc 0.52 loss 1.2464 with data size 243 
val epoch 30: acc 0.49 loss 1.3035 with data size 80 
==============================================
Epoch: 31 
train epoch 31: acc 0.52 loss 1.2144 with data size 243 
val epoch 31: acc 0.49 loss 1.3071 with data size 80 
==============================================
Epoch: 32 
train epoch 32: acc 0.52 loss 1.2213 with data size 243 
val epoch 32: acc 0.51 loss 1.3013 with data size 80 
==============================================
Epoch: 33 
train epoch 33: acc 0.52 loss 1.2353 with data size 243 
val epoch 33: acc 0.51 loss 1.3202 with data size 80 
==============================================
Epoch: 34 
train epoch 34: acc 0.52 loss 1.2244 with data size 243 
val epoch 34: acc 0.51 loss 1.3168 with data size 80 
==============================================
Epoch: 35 
train epoch 35: acc 0.51 loss 1.2343 with data size 243 
val epoch 35: acc 0.49 loss 1.3517 with data size 80 
==============================================
Epoch: 36 
train epoch 36: acc 0.53 loss 1.2085 with data size 243 
val epoch 36: acc 0.55 loss 1.3048 with data size 80 
==============================================
Epoch: 37 
train epoch 37: acc 0.52 loss 1.2093 with data size 243 
val epoch 37: acc 0.53 loss 1.2884 with data size 80 
==============================================
Epoch: 38 
train epoch 38: acc 0.52 loss 1.2242 with data size 243 
val epoch 38: acc 0.54 loss 1.3203 with data size 80 
==============================================
Epoch: 39 
train epoch 39: acc 0.51 loss 1.2366 with data size 243 
val epoch 39: acc 0.51 loss 1.3057 with data size 80 
==============================================
Epoch: 40 
created new Adam optimizer with LR: 0.0001train epoch 40: acc 0.53 loss 1.2200 with data size 243 
val epoch 40: acc 0.54 loss 1.2857 with data size 80 
==============================================
Epoch: 41 
train epoch 41: acc 0.53 loss 1.2206 with data size 243 
val epoch 41: acc 0.51 loss 1.3092 with data size 80 
==============================================
Epoch: 42 
train epoch 42: acc 0.53 loss 1.2150 with data size 243 
val epoch 42: acc 0.49 loss 1.3121 with data size 80 
==============================================
Epoch: 43 
train epoch 43: acc 0.53 loss 1.2133 with data size 243 
val epoch 43: acc 0.50 loss 1.2836 with data size 80 
==============================================
Epoch: 44 
train epoch 44: acc 0.52 loss 1.2146 with data size 243 
val epoch 44: acc 0.50 loss 1.3175 with data size 80 
==============================================
Epoch: 45 
train epoch 45: acc 0.51 loss 1.2146 with data size 243 
val epoch 45: acc 0.49 loss 1.3156 with data size 80 
==============================================
Epoch: 46 
train epoch 46: acc 0.54 loss 1.2058 with data size 243 
val epoch 46: acc 0.53 loss 1.3031 with data size 80 
==============================================
Epoch: 47 
train epoch 47: acc 0.53 loss 1.2121 with data size 243 
val epoch 47: acc 0.50 loss 1.3025 with data size 80 
==============================================
Epoch: 48 
train epoch 48: acc 0.53 loss 1.2156 with data size 243 
val epoch 48: acc 0.51 loss 1.3089 with data size 80 
==============================================
Epoch: 49 
train epoch 49: acc 0.54 loss 1.2022 with data size 243 
val epoch 49: acc 0.51 loss 1.3094 with data size 80 
==============================================
Epoch: 50 
train epoch 50: acc 0.54 loss 1.2102 with data size 243 
val epoch 50: acc 0.50 loss 1.2673 with data size 80 
==============================================
Epoch: 51 
train epoch 51: acc 0.52 loss 1.2195 with data size 243 
val epoch 51: acc 0.50 loss 1.2821 with data size 80 
==============================================
Epoch: 52 
train epoch 52: acc 0.52 loss 1.2012 with data size 243 
val epoch 52: acc 0.49 loss 1.3238 with data size 80 
==============================================
Epoch: 53 
train epoch 53: acc 0.53 loss 1.2218 with data size 243 
val epoch 53: acc 0.50 loss 1.3080 with data size 80 
==============================================
Epoch: 54 
train epoch 54: acc 0.53 loss 1.2259 with data size 243 
val epoch 54: acc 0.53 loss 1.2573 with data size 80 
==============================================
Epoch: 55 
train epoch 55: acc 0.53 loss 1.1963 with data size 243 
val epoch 55: acc 0.47 loss 1.3057 with data size 80 
==============================================
Epoch: 56 
train epoch 56: acc 0.52 loss 1.2170 with data size 243 
val epoch 56: acc 0.53 loss 1.3124 with data size 80 
==============================================
Epoch: 57 
train epoch 57: acc 0.52 loss 1.2115 with data size 243 
val epoch 57: acc 0.50 loss 1.3484 with data size 80 
==============================================
Epoch: 58 
train epoch 58: acc 0.52 loss 1.2117 with data size 243 
val epoch 58: acc 0.50 loss 1.2956 with data size 80 
==============================================
Epoch: 59 
train epoch 59: acc 0.52 loss 1.2210 with data size 243 
val epoch 59: acc 0.50 loss 1.3059 with data size 80 
==============================================
Epoch: 60 
created new Adam optimizer with LR: 1.0000000000000003e-05train epoch 60: acc 0.50 loss 1.2125 with data size 243 
val epoch 60: acc 0.47 loss 1.3446 with data size 80 
==============================================
Epoch: 61 
train epoch 61: acc 0.53 loss 1.1967 with data size 243 
val epoch 61: acc 0.53 loss 1.2814 with data size 80 
==============================================
Epoch: 62 
train epoch 62: acc 0.53 loss 1.2154 with data size 243 
val epoch 62: acc 0.49 loss 1.3326 with data size 80 
==============================================
Epoch: 63 
train epoch 63: acc 0.52 loss 1.2062 with data size 243 
val epoch 63: acc 0.51 loss 1.2805 with data size 80 
==============================================
Epoch: 64 
train epoch 64: acc 0.52 loss 1.2151 with data size 243 
val epoch 64: acc 0.47 loss 1.2921 with data size 80 
==============================================
Epoch: 65 
train epoch 65: acc 0.53 loss 1.2034 with data size 243 
val epoch 65: acc 0.51 loss 1.2979 with data size 80 
==============================================
Epoch: 66 
train epoch 66: acc 0.54 loss 1.2178 with data size 243 
val epoch 66: acc 0.50 loss 1.2857 with data size 80 
==============================================
Epoch: 67 
train epoch 67: acc 0.52 loss 1.2280 with data size 243 
val epoch 67: acc 0.53 loss 1.3083 with data size 80 
==============================================
Epoch: 68 
train epoch 68: acc 0.51 loss 1.2240 with data size 243 
val epoch 68: acc 0.53 loss 1.2640 with data size 80 
==============================================
Epoch: 69 
train epoch 69: acc 0.54 loss 1.2053 with data size 243 
val epoch 69: acc 0.51 loss 1.3059 with data size 80 
==============================================
Epoch: 70 
train epoch 70: acc 0.53 loss 1.2301 with data size 243 
val epoch 70: acc 0.51 loss 1.3022 with data size 80 
==============================================
Epoch: 71 
train epoch 71: acc 0.52 loss 1.2151 with data size 243 
val epoch 71: acc 0.53 loss 1.3184 with data size 80 
==============================================
Epoch: 72 
train epoch 72: acc 0.54 loss 1.2082 with data size 243 
val epoch 72: acc 0.50 loss 1.3355 with data size 80 
==============================================
Epoch: 73 
train epoch 73: acc 0.53 loss 1.2086 with data size 243 
val epoch 73: acc 0.51 loss 1.3123 with data size 80 
==============================================
Epoch: 74 
train epoch 74: acc 0.54 loss 1.2077 with data size 243 
val epoch 74: acc 0.53 loss 1.2729 with data size 80 
==============================================
Epoch: 75 
train epoch 75: acc 0.53 loss 1.2100 with data size 243 
val epoch 75: acc 0.46 loss 1.3517 with data size 80 
==============================================
Epoch: 76 
train epoch 76: acc 0.52 loss 1.2242 with data size 243 
val epoch 76: acc 0.47 loss 1.3256 with data size 80 
==============================================
Epoch: 77 
train epoch 77: acc 0.54 loss 1.2004 with data size 243 
val epoch 77: acc 0.45 loss 1.3134 with data size 80 
==============================================
Epoch: 78 
train epoch 78: acc 0.53 loss 1.1958 with data size 243 
val epoch 78: acc 0.51 loss 1.2834 with data size 80 
==============================================
Epoch: 79 
train epoch 79: acc 0.53 loss 1.1943 with data size 243 
val epoch 79: acc 0.54 loss 1.3002 with data size 80 
==============================================
Epoch: 80 
created new Adam optimizer with LR: 1.0000000000000002e-06train epoch 80: acc 0.54 loss 1.2036 with data size 243 
val epoch 80: acc 0.50 loss 1.2806 with data size 80 
==============================================
Epoch: 81 
train epoch 81: acc 0.54 loss 1.1975 with data size 243 
val epoch 81: acc 0.50 loss 1.3064 with data size 80 
==============================================
Epoch: 82 
train epoch 82: acc 0.53 loss 1.2073 with data size 243 
val epoch 82: acc 0.53 loss 1.2822 with data size 80 
==============================================
Epoch: 83 
train epoch 83: acc 0.52 loss 1.2129 with data size 243 
val epoch 83: acc 0.50 loss 1.2953 with data size 80 
==============================================
Epoch: 84 
train epoch 84: acc 0.52 loss 1.2267 with data size 243 
val epoch 84: acc 0.49 loss 1.3059 with data size 80 
==============================================
Epoch: 85 
train epoch 85: acc 0.54 loss 1.2099 with data size 243 
val epoch 85: acc 0.49 loss 1.2942 with data size 80 
==============================================
Epoch: 86 
train epoch 86: acc 0.52 loss 1.2140 with data size 243 
val epoch 86: acc 0.51 loss 1.2732 with data size 80 
==============================================
Epoch: 87 
train epoch 87: acc 0.52 loss 1.2237 with data size 243 
val epoch 87: acc 0.49 loss 1.3263 with data size 80 
==============================================
Epoch: 88 
train epoch 88: acc 0.54 loss 1.2067 with data size 243 
val epoch 88: acc 0.45 loss 1.3507 with data size 80 
==============================================
Epoch: 89 
train epoch 89: acc 0.53 loss 1.1954 with data size 243 
val epoch 89: acc 0.49 loss 1.2991 with data size 80 
==============================================
Epoch: 90 
train epoch 90: acc 0.53 loss 1.2040 with data size 243 
val epoch 90: acc 0.53 loss 1.3085 with data size 80 
==============================================
Epoch: 91 
train epoch 91: acc 0.53 loss 1.2121 with data size 243 
val epoch 91: acc 0.54 loss 1.2744 with data size 80 
==============================================
Epoch: 92 
train epoch 92: acc 0.51 loss 1.2130 with data size 243 
val epoch 92: acc 0.51 loss 1.3138 with data size 80 
==============================================
Epoch: 93 
train epoch 93: acc 0.52 loss 1.2307 with data size 243 
val epoch 93: acc 0.49 loss 1.3069 with data size 80 
==============================================
Epoch: 94 
train epoch 94: acc 0.53 loss 1.2099 with data size 243 
val epoch 94: acc 0.51 loss 1.3043 with data size 80 
==============================================
Epoch: 95 
train epoch 95: acc 0.53 loss 1.2174 with data size 243 
val epoch 95: acc 0.51 loss 1.2764 with data size 80 
==============================================
Epoch: 96 
train epoch 96: acc 0.52 loss 1.2045 with data size 243 
val epoch 96: acc 0.46 loss 1.3044 with data size 80 
==============================================
Epoch: 97 
train epoch 97: acc 0.53 loss 1.2138 with data size 243 
val epoch 97: acc 0.50 loss 1.3056 with data size 80 
==============================================
Epoch: 98 
train epoch 98: acc 0.53 loss 1.2129 with data size 243 
val epoch 98: acc 0.50 loss 1.2895 with data size 80 
==============================================
Epoch: 99 
train epoch 99: acc 0.51 loss 1.2210 with data size 243 
val epoch 99: acc 0.55 loss 1.2853 with data size 80 
==============================================
Epoch: 100 
created new Adam optimizer with LR: 1.0000000000000002e-07train epoch 100: acc 0.52 loss 1.2114 with data size 243 
val epoch 100: acc 0.51 loss 1.2912 with data size 80 
==============================================
